{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4870b9e-470e-4371-8ba2-8eb339070014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65f892b-5920-4259-952f-4096241389c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./results/all/HumanF-MarkrAI/pub-llama-13B-v3',\n",
       " './results/all/42MARU/GenAI-llama2-ko-en-platypus',\n",
       " './results/all/jyoung105/KoR-Orca-Platypus-13B-neft',\n",
       " './results/all/kyujinpy/KoR-Orca-Platypus-13B']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpts = glob('./results/all/*/*')\n",
    "ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f860f2-2f91-482d-9a83-9b2b9bc6c142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HumanF-MarkrAI/pub-llama-13B-v3',\n",
       " '42MARU/GenAI-llama2-ko-en-platypus',\n",
       " 'jyoung105/KoR-Orca-Platypus-13B-neft',\n",
       " 'kyujinpy/KoR-Orca-Platypus-13B']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['/'.join(i.split('/')[-2:]) for i in ckpts]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc60934-a089-49d0-9873-ac6aaccfee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ac548b-51c6-464f-a15c-96518f7a9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('./results/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70842758-a22b-4144-9bd2-cfa606a47560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/jupyter/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fd4d5e4-8926-443c-84fb-575d3c3b68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES='###' python main.py --model hf-causal-experimental --model_args pretrained=HumanF-MarkrAI/pub-llama-13B-v3,use_accelerate=true --tasks kobest_hellaswag,kobest_copa,kobest_boolq,kobest_sentineg,kohatespeech,kohatespeech_apeach,kohatespeech_gen_bias,korunsmile,nsmc,pawsx_ko --num_fewshot 50 --no_cache --batch_size 4 --output_path results/all/HumanF-MarkrAI/pub-llama-13B-v3/50_shot.json\n",
      "CUDA_VISIBLE_DEVICES='###' python main.py --model hf-causal-experimental --model_args pretrained=42MARU/GenAI-llama2-ko-en-platypus,use_accelerate=true --tasks kobest_hellaswag,kobest_copa,kobest_boolq,kobest_sentineg,kohatespeech,kohatespeech_apeach,kohatespeech_gen_bias,korunsmile,nsmc,pawsx_ko --num_fewshot 50 --no_cache --batch_size 4 --output_path results/all/42MARU/GenAI-llama2-ko-en-platypus/50_shot.json\n",
      "CUDA_VISIBLE_DEVICES='###' python main.py --model hf-causal-experimental --model_args pretrained=kyujinpy/KoR-Orca-Platypus-13B,use_accelerate=true --tasks kobest_hellaswag,kobest_copa,kobest_boolq,kobest_sentineg,kohatespeech,kohatespeech_apeach,kohatespeech_gen_bias,korunsmile,nsmc,pawsx_ko --num_fewshot 50 --no_cache --batch_size 4 --output_path results/all/kyujinpy/KoR-Orca-Platypus-13B/50_shot.json\n"
     ]
    }
   ],
   "source": [
    "RESULT_DIR='results/all'\n",
    "TASKS='kobest_hellaswag,kobest_copa,kobest_boolq,kobest_sentineg,kohatespeech,kohatespeech_apeach,kohatespeech_gen_bias,korunsmile,nsmc,pawsx_ko'\n",
    "for model in models:\n",
    "    for n in [0, 5, 10, 50]:\n",
    "        if (BASE_DIR / model / f'{n}_shot.json').exists():\n",
    "            continue\n",
    "        else:\n",
    "            print(f'''CUDA_VISIBLE_DEVICES='###' python main.py \\\n",
    "--model hf-causal-experimental \\\n",
    "--model_args pretrained={model},use_accelerate=true \\\n",
    "--tasks {TASKS} \\\n",
    "--num_fewshot {n} \\\n",
    "--no_cache \\\n",
    "--batch_size 4 \\\n",
    "--output_path {RESULT_DIR}/{model}/{n}_shot.json''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ffe67-de0c-4d46-bd1b-7a5be84e99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES='0' python main.py --model hf-causal-experimental --model_args pretrained=HumanF-MarkrAI/pub-llama-13B-v3,use_accelerate=true --tasks kobest_hellaswag,kobest_copa,kobest_boolq,kobest_sentineg,kohatespeech,kohatespeech_apeach,kohatespeech_gen_bias,korunsmile,nsmc,pawsx_ko --num_fewshot 50 --no_cache --batch_size 4 --output_path results/all/HumanF-MarkrAI/pub-llama-13B-v3/50_shot.json\n",
    "CUDA_VISIBLE_DEVICES='1' python main.py --model hf-causal-experimental --model_args pretrained=42MARU/GenAI-llama2-ko-en-platypus,use_accelerate=true --tasks kobest_hellaswag,kobest_copa,kobest_boolq,kobest_sentineg,kohatespeech,kohatespeech_apeach,kohatespeech_gen_bias,korunsmile,nsmc,pawsx_ko --num_fewshot 50 --no_cache --batch_size 4 --output_path results/all/42MARU/GenAI-llama2-ko-en-platypus/50_shot.json\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
